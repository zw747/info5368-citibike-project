{
 "cells": [
  {
   "cell_type": "code",
   "id": "712e16db-4385-452e-a36f-5cf0532c4a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:05:34.569125Z",
     "start_time": "2025-05-14T16:05:34.545182Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import itertools  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, learning_rate=0.001, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.cost_history = []\n",
    "        self.W = None\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "        X_transformed = self.transform(X)\n",
    "\n",
    "        if hasattr(self, 'mean') and hasattr(self, 'std'):\n",
    "            X_normalized = (X_transformed - self.mean) / self.std\n",
    "        else:\n",
    "            X_normalized = X_transformed\n",
    "\n",
    "        X_with_ones = np.c_[np.ones((X_normalized.shape[0], 1)), X_normalized]\n",
    "        W = self.W\n",
    "        if W.ndim == 1:\n",
    "            W = W.reshape(-1, 1)\n",
    "\n",
    "        predictions = X_with_ones.dot(W)\n",
    "\n",
    "        return predictions.flatten()\n",
    "\n",
    "    def update_weights(self):\n",
    "        if self.W.ndim == 1:\n",
    "            self.W = self.W.reshape(-1, 1)\n",
    "\n",
    "        Y_pred = self.X_with_ones.dot(self.W)\n",
    "\n",
    "        error = Y_pred - self.Y\n",
    "\n",
    "        gradient = (2 * self.X_with_ones.T.dot(error)) / self.num_examples\n",
    "        self.W -= self.learning_rate * gradient\n",
    "\n",
    "        self.cost_history.append(np.mean((error) ** 2))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        self.original_n_features = X.shape[1]\n",
    "\n",
    "        X_transformed = self.transform(X)\n",
    "\n",
    "        self.mean = np.mean(X_transformed, axis=0)\n",
    "        self.std = np.std(X_transformed, axis=0)\n",
    "        self.std[self.std == 0] = 1  \n",
    "        X_normalized = (X_transformed - self.mean) / self.std\n",
    "\n",
    "        self.X_with_ones = np.c_[np.ones((X_normalized.shape[0], 1)), X_normalized]\n",
    "\n",
    "        self.W = np.zeros((self.X_with_ones.shape[1], 1))\n",
    "        \n",
    "        self.X = X_normalized\n",
    "        self.Y = Y.reshape(-1, 1)\n",
    "        self.num_examples = X_normalized.shape[0]\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            self.update_weights()\n",
    "\n",
    "            if i > 1 and abs(self.cost_history[-1] - self.cost_history[-2]) < 1e-6:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_r2(self, X, Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        SS_res = np.sum((Y - Y_pred) ** 2)\n",
    "        SS_tot = np.sum((Y - np.mean(Y)) ** 2)\n",
    "        r2 = 1 - SS_res / SS_tot\n",
    "\n",
    "        return r2\n",
    "\n",
    "    def get_mse(self, X, Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "    def get_mae(self, X, Y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = np.array(Y)\n",
    "        num_examples = len(y_true)\n",
    "        error = (np.sum(np.abs(y_pred - y_true))) / num_examples\n",
    "        return error\n",
    "\n",
    "\n",
    "class PolynomialRegression(LinearRegression):\n",
    "    def __init__(self, degree=2, learning_rate=0.01, num_iterations=1000):\n",
    "        self.degree = degree\n",
    "        super().__init__(learning_rate, num_iterations)\n",
    "\n",
    "    def transform(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "        num_examples, num_features = X.shape\n",
    "        features = []\n",
    "\n",
    "        for j in range(0, self.degree + 1):\n",
    "            if j == 0:\n",
    "                continue\n",
    "            for combinations in itertools.combinations_with_replacement(range(num_features), j):\n",
    "                feature = np.ones(num_examples)\n",
    "                for each_combination in combinations:\n",
    "                    feature = feature * X[:, each_combination]\n",
    "                features.append(feature.reshape(-1, 1))\n",
    "\n",
    "        if features:\n",
    "            X_transform = np.concatenate(features, axis=1)\n",
    "        else:\n",
    "            X_transform = X\n",
    "\n",
    "        return X_transform\n",
    "\n",
    "\n",
    "class KNN(object):\n",
    "    def __init__(self, k=20, metric='Minkowski', p=1):\n",
    "        self.model_name = 'K Nearest Neighbor Regressor'\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def euclidean_distance(self, X, query):\n",
    "        try:\n",
    "            distances = []\n",
    "            for q in query:\n",
    "                dist = np.sqrt(np.sum((X - q) ** 2, axis=1))\n",
    "                distances.append(dist)\n",
    "            return np.array(distances)\n",
    "        except ValueError as err:\n",
    "            print(f\"Error in euclidean_distance: {str(err)}\")\n",
    "            return None\n",
    "\n",
    "    def manhattan_distance(self, X, query):\n",
    "        try:\n",
    "            distances = []\n",
    "            for q in query:\n",
    "                dist = np.sum(np.abs(X - q), axis=1)\n",
    "                distances.append(dist)\n",
    "            return np.array(distances)\n",
    "        except ValueError as err:\n",
    "            print(f\"Error in manhattan_distance: {str(err)}\")\n",
    "            return None\n",
    "\n",
    "    def minkowski_distance(self, X, query):\n",
    "        try:\n",
    "            distances = []\n",
    "            for q in query:\n",
    "                dist = np.power(np.sum(np.power(np.abs(X - q), self.p), axis=1), 1 / self.p)\n",
    "                distances.append(dist)\n",
    "            return np.array(distances)\n",
    "        except ValueError as err:\n",
    "            print(f\"Error in minkowski_distance: {str(err)}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_distance(self, X, query):\n",
    "        if self.metric == 'Euclidean':\n",
    "            return self.euclidean_distance(X, query)\n",
    "        elif self.metric == 'Manhattan':\n",
    "            return self.manhattan_distance(X, query)\n",
    "        elif self.metric == 'Minkowski':\n",
    "            return self.minkowski_distance(X, query)\n",
    "        else:\n",
    "            raise ValueError(f\"Not Supported metrics: {self.metric}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "        self.mean = np.mean(self.X_train, axis=0)\n",
    "        self.std = np.std(self.X_train, axis=0)\n",
    "        self.std[self.std == 0] = 1\n",
    "\n",
    "        self.X_train_normalized = (self.X_train - self.mean) / self.std\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def kneighbors(self, X, k=None):\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "\n",
    "        X = np.array(X)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        X_normalized = (X - self.mean) / self.std\n",
    "\n",
    "        distances = self.calculate_distance(self.X_train_normalized, X_normalized)\n",
    "\n",
    "        all_indices = []\n",
    "        all_distances = []\n",
    "\n",
    "        for i, dist in enumerate(distances):\n",
    "            sorted_indices = np.argsort(dist)[:k]\n",
    "            all_indices.append(sorted_indices)\n",
    "            all_distances.append(dist[sorted_indices])\n",
    "\n",
    "        return np.array(all_distances), np.array(all_indices)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        _, neighbor_indices = self.kneighbors(X)\n",
    "\n",
    "        predictions = []\n",
    "        for indices in neighbor_indices:\n",
    "            predictions.append(np.mean(self.y_train[indices]))\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def get_r2(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "    def get_mse(self, X, Y):\n",
    "        Y_pred = self.predict(X)\n",
    "        Y = np.array(Y)\n",
    "        return np.mean((Y - Y_pred) ** 2)\n",
    "\n",
    "    def get_mae(self, X, Y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = np.array(Y)\n",
    "        num_examples = len(y_true)\n",
    "        error = (np.sum(np.abs(y_pred - y_true))) / num_examples\n",
    "        return error\n",
    "\n",
    "\n",
    "def sample_and_merge_datasets(data_folder, sample_ratio=0.1, min_samples=1000, max_samples=100000):\n",
    "    print(\"start sample and merge datasets...\")\n",
    "    print(f\"sample ratio: {sample_ratio:.1%}\")\n",
    "    \n",
    "    files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "    \n",
    "    print(f\"find {len(files)} csv files\")\n",
    "    \n",
    "    sampled_dfs = []\n",
    "    total_original_size = 0\n",
    "    total_sampled_size = 0\n",
    "    \n",
    "    for file in files:\n",
    "        df_sample = pd.read_csv(file, nrows=10)\n",
    "\n",
    "        file_size = os.path.getsize(file)\n",
    "        estimated_rows = int(file_size / (len(df_sample.to_csv().encode('utf-8')) / 10))\n",
    "\n",
    "        sample_size = int(estimated_rows * sample_ratio)\n",
    "        sample_size = max(min_samples, min(max_samples, sample_size))\n",
    "\n",
    "        if estimated_rows <= sample_size:\n",
    "            df = pd.read_csv(file)\n",
    "        else:\n",
    "            skip_rows = np.random.randint(0, estimated_rows - sample_size + 1)\n",
    "            df = pd.read_csv(file, skiprows=range(1, skip_rows + 1), nrows=sample_size)\n",
    "    \n",
    "        sampled_dfs.append(df)\n",
    "        total_original_size += estimated_rows\n",
    "        total_sampled_size += len(df)\n",
    "            \n",
    "    combined_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    \n",
    "    print(f\"original total size: {total_original_size:,}\")\n",
    "    print(f\"size after sample: {total_sampled_size:,}\")\n",
    "    print(f\"actual sample ratio: {total_sampled_size/total_original_size:.1%}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    print(\"preprocess data...\")\n",
    "    \n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], format='mixed', errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], format='mixed', errors='coerce')\n",
    "    \n",
    "    df = df.dropna(subset=['start_station_id', 'end_station_id', 'started_at', 'ended_at'])\n",
    "    \n",
    "    df['ride_duration_min'] = (df['ended_at'] - df['started_at']).dt.total_seconds() / 60\n",
    "    \n",
    "    df = df[(df['ride_duration_min'] > 0) & (df['ride_duration_min'] < 1440)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_station_features(df, station_id):\n",
    "    station_starts = df[df['start_station_id'] == station_id]\n",
    "    station_ends = df[df['end_station_id'] == station_id]\n",
    "    \n",
    "    if station_starts.empty and station_ends.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if not station_starts.empty:\n",
    "        station_name = station_starts['start_station_name'].iloc[0]\n",
    "    else:\n",
    "        station_name = station_ends['end_station_name'].iloc[0]\n",
    "    \n",
    "    start_date = df['started_at'].min().floor('h')\n",
    "    end_date = df['started_at'].max().ceil('h')\n",
    "    hours = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "    \n",
    "    flow_data = []\n",
    "    for hour in hours:\n",
    "        hour_end = hour + timedelta(hours=1)\n",
    "        \n",
    "        out_flow = station_starts[(station_starts['started_at'] >= hour) & \n",
    "            (station_starts['started_at'] < hour_end)].shape[0]\n",
    "        \n",
    "        in_flow = station_ends[(station_ends['ended_at'] >= hour) & \n",
    "            (station_ends['ended_at'] < hour_end)].shape[0]\n",
    "        \n",
    "        total_flow = in_flow + out_flow\n",
    "        \n",
    "        flow_data.append({\n",
    "            'datetime': hour,\n",
    "            'station_id': station_id,\n",
    "            'station_name': station_name,\n",
    "            'total_flow': total_flow,\n",
    "            'hour': hour.hour,\n",
    "            'day_of_week': hour.dayofweek,\n",
    "            'is_weekend': 1 if hour.dayofweek >= 5 else 0,\n",
    "        })\n",
    "    \n",
    "    flow_df = pd.DataFrame(flow_data)\n",
    "    \n",
    "    flow_df = flow_df.sort_values('datetime')\n",
    "    flow_df['previous_1h_flow'] = flow_df['total_flow'].shift(1).fillna(0)\n",
    "    \n",
    "    past_24h_flows = []\n",
    "    for i in range(len(flow_df)):\n",
    "        past_flows = flow_df['total_flow'].iloc[max(0, i-24):i]\n",
    "        avg_flow = past_flows.mean() if len(past_flows) > 0 else 0\n",
    "        past_24h_flows.append(avg_flow)\n",
    "    flow_df['past_day_avg_flow'] = past_24h_flows\n",
    "    \n",
    "    flow_df['previous_week_flow'] = flow_df['total_flow'].shift(24*7).fillna(0)\n",
    "    \n",
    "    return flow_df\n",
    "\n",
    "\n",
    "def analyze_single_station_with_multiple_algorithms(station_df, station_id, features, target):\n",
    "    total_flow = station_df[target].sum()\n",
    "    non_zero_count = (station_df[target] > 0).sum()\n",
    "    avg_flow = station_df[target].mean()\n",
    "    max_flow = station_df[target].max()\n",
    "    \n",
    "    X = station_df[features]\n",
    "    y = station_df[target]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    algorithms = {\n",
    "        'Linear Regression': LinearRegression(learning_rate=0.001, num_iterations=3000),\n",
    "        'Polynomial Regression (degree=2)': PolynomialRegression(degree=2, learning_rate=0.001, num_iterations=3000),\n",
    "        'Polynomial Regression (degree=3)': PolynomialRegression(degree=3, learning_rate=0.001, num_iterations=3000),\n",
    "        'KNN (k=5)': KNN(k=5, metric='Euclidean'),\n",
    "        'KNN (k=10)': KNN(k=10, metric='Euclidean'),\n",
    "        'KNN (k=20)': KNN(k=20, metric='Euclidean'),\n",
    "        'KNN Manhattan (k=10)': KNN(k=10, metric='Manhattan'),\n",
    "        'KNN Minkowski (k=10, p=2)': KNN(k=10, metric='Minkowski', p=2)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    station_name = station_df['station_name'].iloc[0] if 'station_name' in station_df.columns else f\"Station {station_id}\"\n",
    "    \n",
    "    for name, model in algorithms.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            start_time = time.time()\n",
    "            y_pred = model.predict(X_test)\n",
    "            predict_time = time.time() - start_time\n",
    "            \n",
    "            mse = model.get_mse(X_test, y_test)\n",
    "            mae = model.get_mae(X_test, y_test)\n",
    "            r2 = model.get_r2(X_test, y_test)\n",
    "            \n",
    "            result = {'station_id': station_id,\n",
    "                'station_name': station_name,\n",
    "                'algorithm': name,\n",
    "                'r2': r2,\n",
    "                'mse': mse,\n",
    "                'rmse': np.sqrt(mse),\n",
    "                'mae': mae,\n",
    "                'train_time': train_time,\n",
    "                'predict_time': predict_time,\n",
    "                'n_train': len(X_train),\n",
    "                'n_test': len(X_test),\n",
    "                'total_flow': total_flow,  \n",
    "                'avg_flow': avg_flow,      \n",
    "                'max_flow': max_flow,      \n",
    "                'non_zero_ratio': non_zero_count/len(station_df)}\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"station {station_id} when running {name} goes error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_multi_algorithm_analysis(data_folder,top,sample_ratio):\n",
    "    output_folder = f\"multi_algorithm_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"=== Multi-Algorithm Comparison Analysis Started ===\")\n",
    "    print(f\"Will compare the following algorithms:\")\n",
    "    print(\"1. Linear Regression\")\n",
    "    print(\"2. Polynomial Regression (degree=2, 3)\")\n",
    "    print(\"3. KNN (k=5, 10, 20)\")\n",
    "    print(\"4. KNN (Manhattan,Euclidean,Minkowski )\")\n",
    "    print(\"\\nNote: Will select top 50 stations by flow for more meaningful results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n1. Data sampling and preprocessing...\")\n",
    "        df = sample_and_merge_datasets(data_folder, sample_ratio)\n",
    "        df = preprocess_data(df)\n",
    "        df.to_csv(os.path.join(output_folder, 'sampled_df.csv'))\n",
    "        \n",
    "        print(\"\\n2. Selecting top 50 stations by flow...\")\n",
    "\n",
    "        start_station_ids = set(df['start_station_id'].dropna())\n",
    "        end_station_ids = set(df['end_station_id'].dropna())\n",
    "        all_station_ids = list(start_station_ids.union(end_station_ids))\n",
    "        \n",
    "        station_flows = {}\n",
    "        print(\"Calculating station flow statistics...\")\n",
    "        \n",
    "        for station_id in all_station_ids:\n",
    "            start_count = (df['start_station_id'] == station_id).sum()\n",
    "            end_count = (df['end_station_id'] == station_id).sum()\n",
    "            total_trips = start_count + end_count\n",
    "            station_flows[station_id] = total_trips\n",
    "        \n",
    "        sorted_stations = sorted(station_flows.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected_stations = [station_id for station_id, _ in sorted_stations[:top]]\n",
    "        \n",
    "        print(f\"Found {len(all_station_ids)} unique stations\")\n",
    "        print(f\"Top 5 stations by flow:\")\n",
    "        for i in range(min(5, len(sorted_stations))):\n",
    "            station_id, flow = sorted_stations[i]\n",
    "            print(f\"  {station_id}: {flow} trips\")\n",
    "        print(f\"Selected top {top} stations by flow: {len(selected_stations)} stations\")\n",
    "        \n",
    "        features = ['hour', 'day_of_week', 'is_weekend', 'previous_1h_flow', 'past_day_avg_flow','previous_week_flow']\n",
    "        target = 'total_flow'\n",
    "        \n",
    "        print(\"\\n3. Starting multi-algorithm analysis...\")\n",
    "        all_results = []\n",
    "        \n",
    "        for i, station_id in enumerate(selected_stations):\n",
    "            station_df = generate_station_features(df, station_id)\n",
    "            \n",
    "            station_results = analyze_single_station_with_multiple_algorithms(\n",
    "                station_df, station_id, features, target)\n",
    "            \n",
    "            all_results.extend(station_results)\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Analyzed {i + 1}/{len(selected_stations)} stations\")\n",
    "        \n",
    "        print(\"\\n4. Results analysis...\")\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        algorithm_stats = results_df.groupby('algorithm').agg({\n",
    "            'r2': ['mean', 'std', 'min', 'max'],\n",
    "            'rmse': ['mean', 'std', 'min', 'max'],\n",
    "            'mae': ['mean', 'std', 'min', 'max'],\n",
    "            'train_time': ['mean', 'std'],\n",
    "            'predict_time': ['mean', 'std']}).round(4)\n",
    "        \n",
    "        print(\"5. Saving results...\")\n",
    "        results_df.to_csv(os.path.join(output_folder, 'detailed_results.csv'), index=False)\n",
    "        algorithm_stats.to_csv(os.path.join(output_folder, 'algorithm_statistics.csv'))\n",
    "        \n",
    "        summary = results_df.groupby('algorithm').agg({\n",
    "            'r2': ['mean', 'count'],\n",
    "            'rmse': 'mean',\n",
    "            'train_time': 'mean',\n",
    "            'predict_time': 'mean'}).round(4)\n",
    "    \n",
    "        summary.columns = ['R²_mean', 'Count', 'RMSE_mean', 'Train_time', 'Predict_time']\n",
    "        summary = summary.sort_values('R²_mean', ascending=False)\n",
    "        summary.to_csv(os.path.join(output_folder, 'algorithm_summary.csv'))\n",
    "        \n",
    "\n",
    "        print(\"\\n=== Analysis Results Summary ===\")\n",
    "        print(\"Best performing algorithms (sorted by R²):\")\n",
    "        print(summary.head())\n",
    "        \n",
    "        return output_folder\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        print(\"Please check data path and file format\")\n",
    "        return None\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"C:\\\\Users\\\\28111\\\\Desktop\\\\bikedata\"\n",
    "    result_folder = run_multi_algorithm_analysis(data_folder,top=3,sample_ratio=0.05)\n",
    "\n",
    "    if result_folder:\n",
    "            print(f\"\\nAnalysis complete! Please check the following files:\")\n",
    "            print(f\"1. Detailed results: {result_folder}\\\\detailed_results.csv\")\n",
    "            print(f\"2. Algorithm statistics: {result_folder}\\\\algorithm_statistics.csv\")\n",
    "            print(f\"3. Algorithm summary: {result_folder}\\\\algorithm_summary.csv\")\n",
    "            print(f\"4. Sampled dataset: {result_folder}\\\\sampled_df.csv\")"
   ],
   "id": "31422cbb7b1b573c"
  },
  {
   "cell_type": "code",
   "id": "80d056fb-f3cb-468e-acda-aa2f31d01575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T16:32:48.865773Z",
     "start_time": "2025-05-14T16:05:38.561944Z"
    }
   },
   "source": "result_folder = run_multi_algorithm_analysis(data_folder,top=3,sample_ratio=0.1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Algorithm Comparison Analysis Started ===\n",
      "Will compare the following algorithms:\n",
      "1. Linear Regression\n",
      "2. Polynomial Regression (degree=2, 3)\n",
      "3. KNN (k=5, 10, 20)\n",
      "4. KNN (Manhattan,Euclidean,Minkowski )\n",
      "\n",
      "Note: Will select top 50 stations by flow for more meaningful results\n",
      "==================================================\n",
      "\n",
      "1. Data sampling and preprocessing...\n",
      "start sample and merge datasets...\n",
      "sample ratio: 10.0%\n",
      "find 42 csv files\n",
      "original total size: 41,282,350\n",
      "size after sample: 3,547,955\n",
      "actual sample ratio: 8.6%\n",
      "preprocess data...\n",
      "\n",
      "2. Selecting top 50 stations by flow...\n",
      "Calculating station flow statistics...\n",
      "Found 4533 unique stations\n",
      "Top 5 stations by flow:\n",
      "  5788.13: 29000 trips\n",
      "  7175.05: 21751 trips\n",
      "  6289.06: 20796 trips\n",
      "  5374.01: 18980 trips\n",
      "  6197.08: 18439 trips\n",
      "Selected top 3 stations by flow: 3 stations\n",
      "\n",
      "3. Starting multi-algorithm analysis...\n",
      "\n",
      "4. Results analysis...\n",
      "5. Saving results...\n",
      "\n",
      "=== Analysis Results Summary ===\n",
      "Best performing algorithms (sorted by R²):\n",
      "                           R²_mean  Count  RMSE_mean  Train_time  Predict_time\n",
      "algorithm                                                                     \n",
      "KNN Manhattan (k=10)        0.8102      3     2.8098      0.0007        0.5960\n",
      "KNN (k=20)                  0.8043      3     2.8543      0.0007        0.6316\n",
      "KNN (k=10)                  0.8003      3     2.8847      0.0003        0.5893\n",
      "KNN Minkowski (k=10, p=2)   0.8003      3     2.8847      0.0000        1.2972\n",
      "KNN (k=5)                   0.7898      3     2.9564      0.0005        0.6172\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "71341da1f5eb38d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
